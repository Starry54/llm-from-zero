{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: numpy in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: requests in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: torch in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (2.1.2+cu118)\n",
      "Requirement already satisfied: tiktoken in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: matplotlib in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: pandas in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: filelock in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\workspace\\env\\anaconda\\envs\\d2l\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy requests torch tiktoken matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2651b5783d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "context_len = 16 # 每个batch的token块长度\n",
    "d_model = 64 # token embeddings vector 的长度\n",
    "num_layers = 8\n",
    "num_heads = 4\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.1\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "eval_iters = 20\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text size: 77919\n",
      "Vocabulary size: 77919\n",
      "The maximum value in the tokenized text is: 100069\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = torch.tensor(encoding.encode(text)) # size of tokenized source text is 77,919\n",
    "vocab_size = len(set(tokenized_text)) # size of vocabulary is 3,771\n",
    "max_token_value = max(tokenized_text)\n",
    "\n",
    "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"The maximum value in the tokenized text is: {max_token_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16]) torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(tokenized_text) * 0.8)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]\n",
    "\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0, high=len(data) - context_len, size=(batch_size,))\n",
    "x_batch = torch.stack([data[idx:idx + context_len] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx + 1: idx + context_len + 1] for idx in idxs])\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP3: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3467, -0.5839,  0.1942,  ...,  1.0553,  0.8381, -0.7491],\n",
       "          [-0.7265,  0.2796,  0.6138,  ...,  0.4791,  1.4515, -0.3293],\n",
       "          [-0.8843, -0.4804,  1.0096,  ...,  0.6763,  1.0791,  0.2051],\n",
       "          ...,\n",
       "          [ 0.6703, -0.4721,  0.2348,  ...,  1.3074, -1.2579, -1.7545],\n",
       "          [ 0.4058,  0.7155, -1.2484,  ..., -0.4716, -1.8187,  0.2622],\n",
       "          [-0.5466,  0.5479, -0.8568,  ..., -2.0618,  0.7363,  1.5494]],\n",
       " \n",
       "         [[ 1.8155, -1.2609,  0.0187,  ...,  2.1571, -1.2867,  1.6914],\n",
       "          [ 0.3175,  2.1064, -0.0922,  ..., -0.3214, -0.4788, -0.1669],\n",
       "          [-1.9643,  0.6743, -0.8320,  ..., -0.2654,  0.8217, -0.5584],\n",
       "          ...,\n",
       "          [ 0.7099,  1.3693, -0.7076,  ..., -1.3573,  1.6040,  0.9203],\n",
       "          [ 1.1325, -0.9729,  0.7417,  ...,  1.2694, -0.5743, -1.3812],\n",
       "          [ 0.4447,  0.9593, -0.4624,  ..., -1.4465,  0.6830,  0.4889]],\n",
       " \n",
       "         [[-0.4976,  0.0382, -0.7235,  ...,  0.9505,  0.9450,  1.8551],\n",
       "          [ 0.6253, -1.1287, -2.4761,  ...,  1.6424, -0.1561, -0.9304],\n",
       "          [-0.8562, -2.4471,  0.9076,  ...,  0.8297, -0.5052,  0.0147],\n",
       "          ...,\n",
       "          [ 0.8231,  0.6719,  0.3348,  ..., -1.5489, -0.8069,  0.9040],\n",
       "          [ 0.0732,  0.2302,  0.5674,  ..., -0.6189,  1.2420, -0.4756],\n",
       "          [-1.3548,  0.8583, -0.8226,  ...,  1.5800, -0.9646,  1.6753]],\n",
       " \n",
       "         [[-0.0306,  0.3051, -0.1997,  ..., -0.7458, -0.2424, -0.4574],\n",
       "          [ 0.2171,  0.4090,  1.1796,  ...,  0.8375,  0.8003, -0.3532],\n",
       "          [ 0.1434,  0.0615, -0.9960,  ...,  0.4135, -0.5170,  0.0431],\n",
       "          ...,\n",
       "          [ 2.6137, -1.2857, -0.7187,  ..., -0.6461, -0.4299,  0.3819],\n",
       "          [-0.1034,  0.7753, -0.6906,  ...,  0.7339,  2.0362,  1.2209],\n",
       "          [ 0.2666, -0.8082, -1.1016,  ...,  0.2824, -0.6161,  1.4144]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[[-0.7265,  0.2796,  0.6138,  ...,  0.4791,  1.4515, -0.3293],\n",
       "          [-0.8843, -0.4804,  1.0096,  ...,  0.6763,  1.0791,  0.2051],\n",
       "          [ 1.3997, -1.2069,  0.3280,  ..., -0.7247,  1.6701, -0.2417],\n",
       "          ...,\n",
       "          [ 0.4058,  0.7155, -1.2484,  ..., -0.4716, -1.8187,  0.2622],\n",
       "          [-0.5466,  0.5479, -0.8568,  ..., -2.0618,  0.7363,  1.5494],\n",
       "          [ 0.9871,  0.6331, -1.5721,  ..., -0.9566,  1.3783,  0.2281]],\n",
       " \n",
       "         [[ 0.3175,  2.1064, -0.0922,  ..., -0.3214, -0.4788, -0.1669],\n",
       "          [-1.9643,  0.6743, -0.8320,  ..., -0.2654,  0.8217, -0.5584],\n",
       "          [ 1.3273, -1.0724, -0.2378,  ..., -0.2312, -0.1408,  1.5658],\n",
       "          ...,\n",
       "          [ 1.1325, -0.9729,  0.7417,  ...,  1.2694, -0.5743, -1.3812],\n",
       "          [ 0.4447,  0.9593, -0.4624,  ..., -1.4465,  0.6830,  0.4889],\n",
       "          [-0.5062, -0.5681, -0.8123,  ..., -1.6292, -0.8667, -0.0737]],\n",
       " \n",
       "         [[ 0.6253, -1.1287, -2.4761,  ...,  1.6424, -0.1561, -0.9304],\n",
       "          [-0.8562, -2.4471,  0.9076,  ...,  0.8297, -0.5052,  0.0147],\n",
       "          [-1.1878, -1.9581, -0.5132,  ...,  1.1159,  0.0754,  0.2283],\n",
       "          ...,\n",
       "          [ 0.0732,  0.2302,  0.5674,  ..., -0.6189,  1.2420, -0.4756],\n",
       "          [-1.3548,  0.8583, -0.8226,  ...,  1.5800, -0.9646,  1.6753],\n",
       "          [ 0.3749, -0.0422,  0.7197,  ...,  0.1141,  0.1620,  0.2443]],\n",
       " \n",
       "         [[ 0.2171,  0.4090,  1.1796,  ...,  0.8375,  0.8003, -0.3532],\n",
       "          [ 0.1434,  0.0615, -0.9960,  ...,  0.4135, -0.5170,  0.0431],\n",
       "          [ 0.5323, -0.1001,  0.1811,  ...,  0.3246,  1.5258, -1.6025],\n",
       "          ...,\n",
       "          [-0.1034,  0.7753, -0.6906,  ...,  0.7339,  2.0362,  1.2209],\n",
       "          [ 0.2666, -0.8082, -1.1016,  ...,  0.2824, -0.6161,  1.4144],\n",
       "          [ 0.3820, -0.9493,  0.0395,  ..., -0.3059,  0.1565, -1.3868]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_lookup_table = nn.Embedding(max_token_value, d_model)\n",
    "\n",
    "x = token_embedding_lookup_table(x_batch.data)\n",
    "y = token_embedding_lookup_table(y_batch.data)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Encoding Look-up Table torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "position_encoding_lookup_table = torch.zeros(context_len, d_model) # 初始化为0 [4, 16, 64]\n",
    "position = torch.arange(0, context_len, dtype=torch.float).unsqueeze(1) # 位置编码， 升维 => [16, 1]\n",
    "\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "print(\"Position Encoding Look-up Table\", position_encoding_lookup_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Input Embedding of x: \n",
      "           0         1         2         3         4         5         6   \\\n",
      "0   0.346711  0.416130  0.194183  2.676231  0.104132  0.317640 -0.716680   \n",
      "1   0.115021  0.819889  1.295355  0.050067  1.152808  0.007295  2.096463   \n",
      "2   0.025032 -0.896510  2.007128  0.645062  1.861388  1.183611  0.626594   \n",
      "3   1.540862 -2.196910  1.106263 -0.879057  1.525128 -0.364159  1.305572   \n",
      "4  -1.026346 -2.520314  0.092276 -1.922128  0.772673 -1.248682  0.811818   \n",
      "5  -0.333637 -0.845062 -3.047244 -0.539240  0.370664 -0.112349  0.087253   \n",
      "6  -0.671321  1.424937 -2.025386 -1.031886 -0.385049 -0.656757  0.465220   \n",
      "7   1.460090  1.632251 -2.407473 -0.108412 -1.577921 -1.534449 -0.767310   \n",
      "8   0.493083 -2.413755 -1.205481  0.105365 -0.475036  0.941739  0.546560   \n",
      "9   0.381502 -0.606034  0.249510  1.915294 -0.984113  2.072770 -2.097719   \n",
      "10 -0.586758 -1.718416  1.647630 -0.431370 -0.642387 -0.391723 -0.296238   \n",
      "11 -0.625085 -0.037802  1.642752  1.460066  0.417635  2.481112 -0.515116   \n",
      "12 -0.137083 -1.089729 -0.385102 -1.420898 -0.049978 -0.465174 -1.718115   \n",
      "13  1.090419  0.435371 -0.083390 -1.558577  2.418297  1.322249  0.850477   \n",
      "14  1.396442  0.852242 -2.127405 -2.404308  0.666056 -1.643007 -0.924739   \n",
      "15  0.103718 -0.211814 -1.824957  1.499077  0.716739  0.161400 -0.793259   \n",
      "\n",
      "          7         8         9   ...        54        55        56        57  \\\n",
      "0   2.721329 -1.379066  1.958478  ...  0.326141  0.918822 -0.974995  0.776366   \n",
      "1   1.836588 -1.455025  2.047307  ...  0.434175  1.833931 -0.511320  1.208292   \n",
      "2   0.867692  0.332192  0.616423  ...  0.095568 -0.311354 -1.522920  1.861448   \n",
      "3  -0.088271  0.942626 -0.624826  ... -0.546487  2.515671  1.224692  1.833670   \n",
      "4   0.063149  1.300849 -0.709820  ...  0.412297  0.811908  0.551316  1.974630   \n",
      "5  -1.396884  1.910200  0.589691  ... -2.392044  0.201838 -0.968136  1.399131   \n",
      "6  -2.319048  0.048269 -0.330767  ... -1.924361  1.770509  0.366548  2.237813   \n",
      "7  -3.344894  0.582778 -0.942104  ... -0.624013  2.197190 -0.286863  1.692157   \n",
      "8  -2.336168 -0.348882 -1.902892  ... -1.072307  1.369699 -0.530627  2.117681   \n",
      "9   0.134769 -0.296123 -2.457148  ...  0.307279  0.597253  0.045292  0.127551   \n",
      "10 -1.070488 -0.842712 -1.117290  ...  0.246956  0.162995 -0.534199 -0.385929   \n",
      "11 -1.015839  0.074801 -1.277103  ...  0.305872 -0.519248 -0.783341  1.396661   \n",
      "12 -0.714581  0.168579 -1.108954  ... -0.903095 -1.738268  0.204137  1.928488   \n",
      "13 -0.733657 -1.358562 -1.389894  ... -0.499943  1.458365  0.897332 -0.018865   \n",
      "14  0.318684 -1.985052 -1.211650  ...  0.635039  3.170543 -0.350654  1.756670   \n",
      "15  1.097708  0.100865  2.525651  ...  1.589034  1.928113 -0.097586  2.889562   \n",
      "\n",
      "          58        59        60        61        62        63  \n",
      "0  -1.208963  1.784498  0.213160  2.055345  0.838057  0.250892  \n",
      "1   0.141251  2.429123  0.510447  1.479065  1.451683  0.670747  \n",
      "2  -0.841794  1.096747 -1.112153  1.676283  1.079349  1.205097  \n",
      "3  -0.403482  1.563554  0.591826  0.275255  1.670505  0.758279  \n",
      "4  -0.939264  0.681914 -0.187975  1.821461 -0.166859  0.659965  \n",
      "5  -0.767064  1.607607 -1.890431  2.642382 -0.155402  0.069573  \n",
      "6  -0.125453  0.737559 -2.455601  1.795864  0.514167  0.152090  \n",
      "7  -0.142867  1.440122 -1.049482  1.068458 -0.757912  0.287046  \n",
      "8  -0.386811  0.388143  1.356780  2.472881  0.264217  1.200112  \n",
      "9   1.358625  2.417308 -1.434693  0.254192 -0.241214  0.542596  \n",
      "10 -0.259560  2.623391  0.565274  0.698465  0.255499  1.528143  \n",
      "11 -0.972843  1.512183  1.175235  1.114063  0.163492  1.244288  \n",
      "12  0.637895  0.500249  0.917552  0.887469 -1.344181  0.950669  \n",
      "13  0.955537 -1.200418  0.384898  2.307354 -1.256187 -0.754517  \n",
      "14  0.634567 -0.359409  0.072614  0.528358 -1.816792  1.262165  \n",
      "15 -1.442999  0.118628  0.953136 -1.061797  0.738310  2.549351  \n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "input_embedding_x = x + position_encoding_lookup_table\n",
    "input_embedding_y = y + position_encoding_lookup_table\n",
    "\n",
    "X = input_embedding_x\n",
    "\n",
    "x_plot = input_embedding_x[0].detach().cpu().numpy()\n",
    "print(\"Final Input Embedding of x: \\n\", pd.DataFrame(x_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP4: Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Query, Key, Value for Multi-head Attention\n",
    "\n",
    "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "# Define Query, Key, Value weight matrices\n",
    "Wq = nn.Linear(d_model, d_model)\n",
    "Wk = nn.Linear(d_model, d_model)\n",
    "Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Wq(query) #[4, 16, 64]\n",
    "Q = Q.view(batch_size, context_len, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "K = Wk(key) #[4, 16, 64]\n",
    "K = K.view(batch_size, context_len, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
    "\n",
    "V = Wv(value) #[4, 16, 64]\n",
    "V = V.view(batch_size, context_len, num_heads, d_model // num_heads)  #[4, 16, 4, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]\n",
    "# The reason is that treat each batch with \"num_heads\" as its first dimension.\n",
    "Q = Q.transpose(1, 2) # [4, 4, 16, 16]\n",
    "K = K.transpose(1, 2) # [4, 4, 16, 16]\n",
    "V = V.transpose(1, 2) # [4, 4, 16, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Q * K ^ T Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the attention score betwee Q and K^T\n",
    "attention_score = torch.matmul(Q, K.transpose(-2, -1)) # 只针对最后两个维度做转置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_score = attention_score / math.sqrt(d_model // num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.271247  0.560849 -0.497451 -0.129847 -0.363947 -0.685937  0.185218   \n",
      "1  -0.045627  0.335207 -0.261123  0.401420 -0.060369  0.060264  0.264576   \n",
      "2  -0.062706  0.149768 -0.373623 -0.014540  0.501353  0.051065  0.528086   \n",
      "3   0.489974  0.889258 -0.396356  0.726592  0.525897 -0.280680 -0.086415   \n",
      "4   0.366094  0.809745  0.008297  0.004744  0.321938 -0.265212 -0.031628   \n",
      "5   0.086670  0.772399 -0.768684  0.338177  0.517480  0.012420  0.335387   \n",
      "6  -0.917973  0.179699 -1.242398  0.295149  0.050819 -0.538681  0.316469   \n",
      "7   0.485319  1.187169 -0.421335  0.609655  0.178571 -0.377602 -0.150101   \n",
      "8   0.442679  1.010227  0.541597  0.759292  0.351397  0.068198 -0.331880   \n",
      "9  -0.378671 -0.184779 -0.172620  0.340819  0.176497  0.439594  0.154800   \n",
      "10  0.634922  1.067505 -0.084768  1.119899  1.075548  0.098884 -0.157741   \n",
      "11  0.714039  0.273180  0.165909 -0.122726  0.084472  0.506430 -0.043268   \n",
      "12  0.522655  0.665469 -0.735746 -0.020036  0.108930 -0.535809 -0.394747   \n",
      "13 -0.416619 -0.336371 -0.045832 -0.401677 -0.210665  0.181484  0.079874   \n",
      "14  0.410861  1.353325 -0.911994  0.392745  0.866432 -0.656386  0.181728   \n",
      "15 -0.060189  0.242830 -0.301406  0.046287  0.258087 -0.577939 -0.037387   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0  -0.109630 -0.582609 -0.399922 -0.526987  0.009846  0.095429 -0.417190   \n",
      "1   0.356827  0.278763 -0.405287  0.052244 -0.403992  0.486559  0.041878   \n",
      "2  -0.215684 -0.363876 -0.507898 -0.526590 -1.015162  0.636724 -0.353700   \n",
      "3  -0.236879 -0.139481  0.180077  0.160038 -0.278772  0.029830  0.025996   \n",
      "4   0.000413 -0.327082  0.007050  0.058440  0.219607  0.311964 -0.035971   \n",
      "5  -0.283661  0.073089 -0.119132 -0.079181  0.178269  0.595736  0.069439   \n",
      "6  -0.602451  0.000879  0.257435  0.622034 -0.252260  0.048440  0.603582   \n",
      "7  -0.018908  0.147505  0.090906  0.280693  0.105192  0.616787 -0.289057   \n",
      "8   0.620946  0.515219  0.133559  0.251447  0.222155 -0.078039  0.416460   \n",
      "9   0.048620  0.835482 -0.098898  0.118051  0.307614 -0.602386  0.551086   \n",
      "10 -0.611069  0.047325 -0.438557 -0.203146 -0.253197 -0.032359 -0.314876   \n",
      "11  0.122951  0.421487  0.044874 -0.884315  0.804205  0.233541 -0.574858   \n",
      "12 -0.308567 -0.607481 -0.350354 -0.753023 -0.458779 -0.123210  0.116115   \n",
      "13  0.125355  0.188627  0.378928 -0.251860  0.577807  0.020881 -0.249761   \n",
      "14 -0.580414 -0.889560 -0.039655 -0.283251  0.058167  0.446691  0.706114   \n",
      "15 -0.346066 -0.720006 -0.087254 -0.501788  0.327810 -0.474141 -0.020896   \n",
      "\n",
      "          14        15  \n",
      "0  -0.294367  0.189911  \n",
      "1   1.014870  0.202047  \n",
      "2   0.629859  0.337414  \n",
      "3   0.100336 -0.437344  \n",
      "4   0.080204  0.044314  \n",
      "5   0.097363  0.496599  \n",
      "6  -0.315566 -0.246330  \n",
      "7   0.113308  0.034956  \n",
      "8   0.846286  0.007500  \n",
      "9   0.633898  0.121890  \n",
      "10 -0.157880  0.322685  \n",
      "11  0.174786  0.648655  \n",
      "12  0.457777  0.435658  \n",
      "13 -0.140071 -0.072628  \n",
      "14 -0.094467  0.229634  \n",
      "15 -0.660642 -0.317020  \n"
     ]
    }
   ],
   "source": [
    "attention_score = torch.matmul(Q, K.transpose(-2, -1) / math.sqrt(d_model // num_heads))\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   0.271247      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1  -0.045627  0.335207      -inf      -inf      -inf      -inf      -inf   \n",
      "2  -0.062706  0.149768 -0.373623      -inf      -inf      -inf      -inf   \n",
      "3   0.489974  0.889258 -0.396356  0.726592      -inf      -inf      -inf   \n",
      "4   0.366094  0.809745  0.008297  0.004744  0.321938      -inf      -inf   \n",
      "5   0.086670  0.772399 -0.768684  0.338177  0.517480  0.012420      -inf   \n",
      "6  -0.917973  0.179699 -1.242398  0.295149  0.050819 -0.538681  0.316469   \n",
      "7   0.485319  1.187169 -0.421335  0.609655  0.178571 -0.377602 -0.150101   \n",
      "8   0.442679  1.010227  0.541597  0.759292  0.351397  0.068198 -0.331880   \n",
      "9  -0.378671 -0.184779 -0.172620  0.340819  0.176497  0.439594  0.154800   \n",
      "10  0.634922  1.067505 -0.084768  1.119899  1.075548  0.098884 -0.157741   \n",
      "11  0.714039  0.273180  0.165909 -0.122726  0.084472  0.506430 -0.043268   \n",
      "12  0.522655  0.665469 -0.735746 -0.020036  0.108930 -0.535809 -0.394747   \n",
      "13 -0.416619 -0.336371 -0.045832 -0.401677 -0.210665  0.181484  0.079874   \n",
      "14  0.410861  1.353325 -0.911994  0.392745  0.866432 -0.656386  0.181728   \n",
      "15 -0.060189  0.242830 -0.301406  0.046287  0.258087 -0.577939 -0.037387   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "2       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "3       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "4       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "5       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "6       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "7  -0.018908      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "8   0.620946  0.515219      -inf      -inf      -inf      -inf      -inf   \n",
      "9   0.048620  0.835482 -0.098898      -inf      -inf      -inf      -inf   \n",
      "10 -0.611069  0.047325 -0.438557 -0.203146      -inf      -inf      -inf   \n",
      "11  0.122951  0.421487  0.044874 -0.884315  0.804205      -inf      -inf   \n",
      "12 -0.308567 -0.607481 -0.350354 -0.753023 -0.458779 -0.123210      -inf   \n",
      "13  0.125355  0.188627  0.378928 -0.251860  0.577807  0.020881 -0.249761   \n",
      "14 -0.580414 -0.889560 -0.039655 -0.283251  0.058167  0.446691  0.706114   \n",
      "15 -0.346066 -0.720006 -0.087254 -0.501788  0.327810 -0.474141 -0.020896   \n",
      "\n",
      "          14       15  \n",
      "0       -inf     -inf  \n",
      "1       -inf     -inf  \n",
      "2       -inf     -inf  \n",
      "3       -inf     -inf  \n",
      "4       -inf     -inf  \n",
      "5       -inf     -inf  \n",
      "6       -inf     -inf  \n",
      "7       -inf     -inf  \n",
      "8       -inf     -inf  \n",
      "9       -inf     -inf  \n",
      "10      -inf     -inf  \n",
      "11      -inf     -inf  \n",
      "12      -inf     -inf  \n",
      "13      -inf     -inf  \n",
      "14 -0.094467     -inf  \n",
      "15 -0.660642 -0.31702  \n"
     ]
    }
   ],
   "source": [
    "attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf'))\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.405926  0.594074  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.336756  0.416478  0.246766  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.239815  0.357506  0.098844  0.303836  0.000000  0.000000  0.000000   \n",
      "4   0.203619  0.317316  0.142373  0.141868  0.194823  0.000000  0.000000   \n",
      "5   0.139601  0.277139  0.059349  0.179522  0.214777  0.129611  0.000000   \n",
      "6   0.064035  0.191923  0.046293  0.215410  0.168716  0.093571  0.220052   \n",
      "7   0.146088  0.294729  0.059001  0.165429  0.107497  0.061639  0.077385   \n",
      "8   0.104471  0.184279  0.115333  0.143383  0.095357  0.071839  0.048151   \n",
      "9   0.057362  0.069635  0.070487  0.117786  0.099938  0.130014  0.097793   \n",
      "10  0.112850  0.173928  0.054947  0.183284  0.175332  0.066024  0.051080   \n",
      "11  0.131966  0.084918  0.076280  0.057156  0.070315  0.107226  0.061883   \n",
      "12  0.147472  0.170111  0.041898  0.085708  0.097506  0.051171  0.058923   \n",
      "13  0.046259  0.050125  0.067024  0.046956  0.056839  0.084130  0.076002   \n",
      "14  0.076939  0.197448  0.020494  0.075557  0.121338  0.026463  0.061183   \n",
      "15  0.068365  0.092562  0.053712  0.076046  0.093985  0.040736  0.069941   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.088233  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.124857  0.112330  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.087941  0.193163  0.075880  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.032462  0.062706  0.038574  0.048813  0.000000  0.000000  0.000000   \n",
      "11  0.073073  0.098494  0.067585  0.026687  0.144418  0.000000  0.000000   \n",
      "12  0.064226  0.047632  0.061598  0.041180  0.055268  0.077306  0.000000   \n",
      "13  0.079538  0.084733  0.102495  0.054545  0.125047  0.071648  0.054659   \n",
      "14  0.028552  0.020959  0.049033  0.038432  0.054072  0.079745  0.103365   \n",
      "15  0.051366  0.035341  0.066539  0.043959  0.100771  0.045191  0.071104   \n",
      "\n",
      "          14       15  \n",
      "0   0.000000  0.00000  \n",
      "1   0.000000  0.00000  \n",
      "2   0.000000  0.00000  \n",
      "3   0.000000  0.00000  \n",
      "4   0.000000  0.00000  \n",
      "5   0.000000  0.00000  \n",
      "6   0.000000  0.00000  \n",
      "7   0.000000  0.00000  \n",
      "8   0.000000  0.00000  \n",
      "9   0.000000  0.00000  \n",
      "10  0.000000  0.00000  \n",
      "11  0.000000  0.00000  \n",
      "12  0.000000  0.00000  \n",
      "13  0.000000  0.00000  \n",
      "14  0.046418  0.00000  \n",
      "15  0.037502  0.05288  \n"
     ]
    }
   ],
   "source": [
    "# Softmax the attention score\n",
    "attention_score = torch.softmax(attention_score, dim=-1)  #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate V Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "A = torch.matmul(attention_score, V)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]\n",
    "A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "Wo = nn.Linear(d_model, d_model)\n",
    "output = Wo(A)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP5: Residual Connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output + X\n",
    "\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP6: Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn.Linear(d_model, d_model * 4)(output)\n",
    "output = nn.ReLU()(output)\n",
    "\n",
    "output = nn.Linear(d_model * 4, d_model)(output)\n",
    "output = torch.dropout(output, p=dropout, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output + X\n",
    "\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP7: Repeat step 4 to 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP8: Output Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1         2         3         4         5         6       \\\n",
      "0  -0.198109 -0.024584 -0.390627 -0.509666 -0.651848 -0.803911  0.021912   \n",
      "1   0.738735  0.621348 -0.293323  0.693907 -0.620037 -0.390120 -0.576853   \n",
      "2   0.461873 -0.118239 -0.102895 -0.944985 -0.255342  0.026568  0.926113   \n",
      "3  -0.594172  0.398664  0.292804 -0.800400 -0.337430 -0.212501  0.310157   \n",
      "4  -0.627845  0.119776  0.000432 -0.299878  0.699299 -0.479650  0.159084   \n",
      "5  -0.001701 -0.636552 -0.205869 -0.914698 -0.654249  0.303629  0.728940   \n",
      "6  -0.556173 -0.015484  0.494321 -0.732024 -0.162457 -0.178356  0.327461   \n",
      "7  -0.812433  0.003836 -0.903373 -0.397297 -0.450321  0.819017  0.168120   \n",
      "8  -1.310561 -0.363689 -0.590449 -0.784727 -0.423371  0.130670 -0.028444   \n",
      "9  -0.341814 -0.570835  0.131305 -0.092621  0.788680  0.772467  0.319421   \n",
      "10  0.292164 -0.595430 -0.032514 -0.172319  0.704094  0.050517 -0.211032   \n",
      "11  0.236014 -0.261912 -0.782080 -0.480625 -0.395049  0.332238  0.043047   \n",
      "12 -0.529474  0.185221 -0.163117  0.080752 -0.161418  0.989779  0.232630   \n",
      "13 -0.807503  0.618209  0.135035  0.148908  1.126865 -0.025793  0.094142   \n",
      "14 -0.942195 -0.076828  0.781585 -0.173694  0.528630 -0.719600  0.088947   \n",
      "15 -0.505915  0.119280 -0.105518 -0.445461  0.290443  0.161432  0.076130   \n",
      "\n",
      "      7         8         9       ...    100059    100060    100061    100062  \\\n",
      "0  -0.220479  0.039764 -0.108272  ...  0.422643 -0.542400 -0.288384  0.489309   \n",
      "1  -0.610312 -0.005386 -0.635274  ... -0.358585 -0.805883 -0.389686 -0.137290   \n",
      "2  -0.454585 -0.137228  0.428997  ...  0.783919  0.032272  0.123375  0.217485   \n",
      "3  -0.417062  0.557524  0.197784  ...  1.346300 -0.294751 -0.072211 -0.559871   \n",
      "4  -1.173773 -0.020348  0.196675  ...  0.335871 -0.506739  0.121237 -1.201865   \n",
      "5  -0.227980 -1.424284  0.170317  ...  0.436821 -0.852558  0.311666  0.167645   \n",
      "6   0.624841 -0.571868 -0.445563  ...  0.371554 -0.177637  0.330660  0.967169   \n",
      "7   0.627974 -0.211449  0.231555  ...  0.471851 -0.179871 -0.121141  0.219998   \n",
      "8  -0.300077  0.613089  0.337455  ...  0.966889 -0.645000 -0.337220  0.264371   \n",
      "9   0.297706 -0.436454  0.547136  ...  0.820867 -0.115552 -0.615278  1.237157   \n",
      "10  0.399044  0.136084 -0.098165  ...  0.569248  0.280060  0.316029 -0.257596   \n",
      "11 -0.154027 -0.215350 -0.862464  ...  1.613596  0.247147  0.190218  0.163263   \n",
      "12 -0.472996 -0.406196 -0.417723  ...  0.581388 -0.324454  0.022837 -1.098135   \n",
      "13 -1.429117 -0.095877 -0.258178  ...  0.538346 -0.396489  0.186412  0.181505   \n",
      "14  0.026578 -0.561324 -0.674583  ...  0.516904  0.151471  0.143191 -0.988389   \n",
      "15 -0.471846 -0.783165 -0.538557  ...  0.044293 -0.162262  0.356479 -0.325504   \n",
      "\n",
      "      100063    100064    100065    100066    100067    100068  \n",
      "0   0.010644 -0.440726 -0.526366  0.622293 -0.418183 -0.550434  \n",
      "1  -0.368307 -0.179193  0.504089  0.876016 -0.690479 -1.898542  \n",
      "2   0.737927  0.075980 -0.161590 -0.048811 -0.602588  0.571079  \n",
      "3   0.528240  0.016780  0.923211  0.509746 -0.820731 -0.576875  \n",
      "4   0.593455  0.964129  0.595328  0.209957  0.412268 -0.578996  \n",
      "5   0.554236  0.688087  0.160057  0.828522  0.391261  0.375971  \n",
      "6   0.258026  0.761336  0.294700  0.176447 -0.762124  0.483515  \n",
      "7   0.406608  0.363350  0.451458 -0.889814 -0.048987 -0.670326  \n",
      "8   1.285893  0.668936  0.956861 -0.396895 -0.400553 -0.502136  \n",
      "9   0.028111  0.167827  0.110621  0.720932 -0.118911 -0.188438  \n",
      "10  0.321118  0.875534  0.178573 -0.282940  0.250577  0.071838  \n",
      "11  0.917744  0.083293 -0.670692  0.890596 -0.012346  0.529791  \n",
      "12  0.783660  0.174414  0.023177  1.882621  0.698803 -0.788091  \n",
      "13 -0.242396  0.380096 -0.076662  0.021750  0.629953 -0.272863  \n",
      "14 -0.222145  0.397878  0.419609 -0.572862  0.446939 -1.365451  \n",
      "15  0.405745  0.412988 -0.103725  0.534419 -0.462917 -0.432683  \n",
      "\n",
      "[16 rows x 100069 columns]\n"
     ]
    }
   ],
   "source": [
    "logits = nn.Linear(d_model, max_token_value)(output)\n",
    "print(pd.DataFrame(logits[0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.9243e-06, 8.2364e-06, 5.7117e-06,  ..., 1.5728e-05,\n",
       "          5.5564e-06, 4.8681e-06],\n",
       "         [1.7647e-05, 1.5692e-05, 6.2870e-06,  ..., 2.0243e-05,\n",
       "          4.2263e-06, 1.2627e-06],\n",
       "         [1.3348e-05, 7.4728e-06, 7.5883e-06,  ..., 8.0100e-06,\n",
       "          4.6040e-06, 1.4888e-05],\n",
       "         ...,\n",
       "         [3.7644e-06, 1.5663e-05, 9.6613e-06,  ..., 8.6265e-06,\n",
       "          1.5848e-05, 6.4252e-06],\n",
       "         [3.3098e-06, 7.8638e-06, 1.8554e-05,  ..., 4.7886e-06,\n",
       "          1.3277e-05, 2.1676e-06],\n",
       "         [5.0956e-06, 9.5217e-06, 7.6047e-06,  ..., 1.4421e-05,\n",
       "          5.3195e-06, 5.4828e-06]],\n",
       "\n",
       "        [[1.9918e-05, 5.9267e-06, 3.7478e-06,  ..., 8.6077e-06,\n",
       "          3.9476e-06, 8.1714e-06],\n",
       "         [4.3524e-06, 1.5325e-05, 6.8608e-06,  ..., 1.6760e-05,\n",
       "          8.0599e-06, 6.1646e-06],\n",
       "         [1.0009e-05, 6.1595e-06, 5.4430e-06,  ..., 1.4058e-05,\n",
       "          8.3669e-06, 6.3447e-06],\n",
       "         ...,\n",
       "         [9.4713e-06, 1.3125e-05, 1.3630e-05,  ..., 8.1968e-06,\n",
       "          1.2910e-05, 5.8490e-06],\n",
       "         [3.8318e-06, 5.5524e-06, 8.8919e-06,  ..., 1.2337e-05,\n",
       "          6.0512e-06, 6.7527e-06],\n",
       "         [3.5080e-06, 1.2648e-05, 1.0049e-05,  ..., 6.0759e-06,\n",
       "          6.7663e-06, 1.0876e-05]],\n",
       "\n",
       "        [[1.1223e-05, 2.1655e-05, 1.4530e-05,  ..., 1.6047e-05,\n",
       "          1.0392e-05, 8.3886e-06],\n",
       "         [1.4387e-05, 5.7244e-06, 7.4620e-06,  ..., 2.5389e-05,\n",
       "          6.3541e-06, 1.3102e-05],\n",
       "         [1.2378e-05, 4.7783e-06, 3.7359e-06,  ..., 3.8530e-06,\n",
       "          2.8820e-06, 5.2977e-06],\n",
       "         ...,\n",
       "         [4.6259e-06, 1.0963e-05, 1.2468e-05,  ..., 2.1710e-05,\n",
       "          8.8671e-06, 5.6400e-06],\n",
       "         [4.1928e-06, 3.6569e-06, 8.5772e-06,  ..., 1.4417e-05,\n",
       "          1.1860e-05, 7.4338e-06],\n",
       "         [9.4218e-06, 5.6236e-06, 8.1912e-06,  ..., 8.6475e-06,\n",
       "          2.1165e-05, 4.5867e-06]],\n",
       "\n",
       "        [[1.0480e-05, 7.4817e-06, 1.5590e-05,  ..., 2.7356e-05,\n",
       "          5.1908e-06, 5.8755e-06],\n",
       "         [1.8820e-05, 7.2717e-06, 8.1463e-06,  ..., 7.2195e-06,\n",
       "          2.5904e-06, 6.7342e-06],\n",
       "         [6.5615e-06, 1.3255e-05, 1.3618e-06,  ..., 4.6722e-06,\n",
       "          5.4085e-06, 6.3296e-06],\n",
       "         ...,\n",
       "         [3.5066e-06, 8.8514e-06, 6.4767e-06,  ..., 9.0931e-06,\n",
       "          8.5955e-06, 9.0738e-06],\n",
       "         [5.5393e-06, 9.1052e-06, 1.7226e-05,  ..., 7.3670e-06,\n",
       "          1.1772e-05, 3.5474e-06],\n",
       "         [3.5425e-06, 4.7882e-06, 6.5140e-06,  ..., 1.4140e-05,\n",
       "          5.4360e-06, 6.4689e-06]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.softmax usually used during inference, during training we use torch.nn.CrossEntropyLoss\n",
    "# but for illustration purpose, we'll use torch.softmax here\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
